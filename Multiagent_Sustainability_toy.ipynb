{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKnWjeasDdNGAPRTeb1rwP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahault/Multi-agent-sustainability/blob/main/Multiagent_Sustainability_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup\n",
        "\n",
        "\n",
        "*   Grid Setup: The environment consists of a 3x3 grid.\n",
        "*   Agent Dynamics: Two agents that can move, communicate, and consume resources.\n",
        "*   Resource Dynamics: Water and food are placed randomly and can deplete and replenish.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uWGa5fvQOXcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pettingzoo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqdCeImROn8Y",
        "outputId": "6bcc5cf0-9e11-42a0-a387-1d5aa6a41c11"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pettingzoo in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (1.25.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.distributions as dist\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils import agent_selector\n",
        "from gym import spaces"
      ],
      "metadata": {
        "id": "ZHCSIXKfXrbB"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size),\n",
        "            nn.Softmax(dim=-1)  # Action probabilities\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size + action_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, actions):\n",
        "        return self.net(torch.cat([x, actions], dim=1))"
      ],
      "metadata": {
        "id": "hKabS05hgnKu"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MADDPGAgent:\n",
        "    def __init__(self, obs_size, action_size, n_agents):\n",
        "        self.actor = Actor(obs_size, action_size).float()\n",
        "        self.critic = Critic(obs_size * n_agents, action_size * n_agents).float()\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.01)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.01)\n",
        "\n",
        "        self.target_actor = Actor(obs_size, action_size).float()\n",
        "        self.target_critic = Critic(obs_size * n_agents, action_size * n_agents).float()\n",
        "\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "    def act(self, state):\n",
        "        grid_state = torch.FloatTensor(state[\"grid\"].flatten()).unsqueeze(0)\n",
        "        action_probs = self.actor(grid_state)\n",
        "        distribution = dist.Categorical(action_probs)\n",
        "        action = distribution.sample().item()\n",
        "        return action\n",
        "\n",
        "    def update(self, batch, agent_id, agents, gamma=0.99, tau=0.01):\n",
        "\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "\n",
        "        # Get the number of agents\n",
        "        n_agents = len(agents)\n",
        "\n",
        "        # Get the current agent's state and action\n",
        "        state = states[agent_id]\n",
        "        action = actions[agent_id]\n",
        "\n",
        "        # Get the next state for the current agent\n",
        "        next_state = next_states[agent_id]\n",
        "\n",
        "        # Get the target actions for the next states using the target actor networks\n",
        "        next_actions = []\n",
        "        for i in range(n_agents):\n",
        "            next_action = agents[i].target_actor(next_states[i])\n",
        "            next_actions.append(next_action)\n",
        "        next_actions = torch.cat(next_actions, dim=1)\n",
        "\n",
        "        # Compute the target Q-value using the target critic network\n",
        "        target_q = agents[agent_id].target_critic(next_state, next_actions)\n",
        "        target_q = rewards[agent_id] + (gamma * target_q * (1 - dones[agent_id]))\n",
        "\n",
        "        # Compute the current Q-value using the critic network\n",
        "        current_q = agents[agent_id].critic(state, actions)\n",
        "\n",
        "        # Compute the critic loss using the TD error (Q-targets - Q-values)\n",
        "        critic_loss = F.mse_loss(current_q, target_q)\n",
        "\n",
        "        # Update the critic network by minimizing the critic loss\n",
        "        agents[agent_id].critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        agents[agent_id].critic_optimizer.step()\n",
        "\n",
        "        # Compute the actor loss using the critic network and the current actions\n",
        "        actor_loss = -agents[agent_id].critic(state, actions).mean()\n",
        "\n",
        "        # Update the actor network by minimizing the actor loss\n",
        "        agents[agent_id].actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        agents[agent_id].actor_optimizer.step()\n",
        "\n",
        "        # Perform soft updates of the target networks for both actor and critic\n",
        "        for target_param, param in zip(agents[agent_id].target_actor.parameters(), agents[agent_id].actor.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "        for target_param, param in zip(agents[agent_id].target_critic.parameters(), agents[agent_id].critic.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
        ""
      ],
      "metadata": {
        "id": "9PgJHfE_XzHi"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MADDPGAgent:\n",
        "    def __init__(self, obs_size, action_size, n_agents):\n",
        "        self.actor = Actor(obs_size, action_size).float()  # Ensure the model is in float\n",
        "        self.critic = Critic(obs_size * n_agents, action_size * n_agents).float()  # Adjust according to the total number of agents\n",
        "\n",
        "        self.position = (0, 0)  # Example initial position\n",
        "        self.water_timer = 3\n",
        "        self.food_timer = 7\n",
        "        # Initialize beliefs, etc.\n",
        "        # Assume beliefs and other state variables are initialized similarly\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.01)  # Example learning rate\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.01)  # Example learning rate\n",
        "\n",
        "        self.target_actor = Actor(obs_size, action_size).float()\n",
        "        self.target_critic = Critic(obs_size * n_agents, action_size * n_agents).float()\n",
        "\n",
        "        # Initialize target networks with the same weights as the original networks\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "    def act(self, state):\n",
        "      grid_state = state[\"grid\"]\n",
        "      grid_state = torch.FloatTensor(grid_state.flatten()).unsqueeze(0)\n",
        "      action_probs = self.actor(grid_state)\n",
        "      action = torch.argmax(action_probs, dim=1).item()  # Select the action with the highest probability\n",
        "      return action\n",
        "\n",
        "    def update(self, batch, agent_id, agents, gamma=0.99, tau=0.01):\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "\n",
        "        # Get the number of agents\n",
        "        n_agents = len(agents)\n",
        "\n",
        "        # Get the current agent's state and action\n",
        "        state = states[agent_id]\n",
        "        action = actions[agent_id]\n",
        "\n",
        "        # Get the next state for the current agent\n",
        "        next_state = next_states[agent_id]\n",
        "\n",
        "        # Get the target actions for the next states using the target actor networks\n",
        "        next_actions = []\n",
        "        for i in range(n_agents):\n",
        "            next_action = agents[i].target_actor(next_states[i])\n",
        "            next_actions.append(next_action)\n",
        "        next_actions = torch.cat(next_actions, dim=1)\n",
        "\n",
        "        # Compute the target Q-value using the target critic network\n",
        "        target_q = agents[agent_id].target_critic(next_state, next_actions)\n",
        "        target_q = rewards[agent_id] + (gamma * target_q * (1 - dones[agent_id]))\n",
        "\n",
        "        # Compute the current Q-value using the critic network\n",
        "        current_q = agents[agent_id].critic(state, actions)\n",
        "\n",
        "        # Compute the critic loss using the TD error (Q-targets - Q-values)\n",
        "        critic_loss = F.mse_loss(current_q, target_q)\n",
        "\n",
        "        # Update the critic network by minimizing the critic loss\n",
        "        agents[agent_id].critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        agents[agent_id].critic_optimizer.step()\n",
        "\n",
        "        # Compute the actor loss using the critic network and the current actions\n",
        "        actor_loss = -agents[agent_id].critic(state, actions).mean()\n",
        "\n",
        "        # Update the actor network by minimizing the actor loss\n",
        "        agents[agent_id].actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        agents[agent_id].actor_optimizer.step()\n",
        "\n",
        "        # Perform soft updates of the target networks for both actor and critic\n",
        "        for target_param, param in zip(agents[agent_id].target_actor.parameters(), agents[agent_id].actor.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "        for target_param, param in zip(agents[agent_id].target_critic.parameters(), agents[agent_id].critic.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "QW0aY6p1u-0P"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        experiences = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ],
      "metadata": {
        "id": "TJO3aYe5Powz"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "4GegEAtwOQIS"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ForagingEnv(AECEnv):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.grid_size = 3\n",
        "        self.n_agents = 2\n",
        "        self.observation_size = self.grid_size * self.grid_size * 3 + 2\n",
        "        self.action_size = 6\n",
        "        self.agents = ['agent_' + str(i) for i in range(self.n_agents)]  # Agent identifiers\n",
        "        self.agent_selector = agent_selector(self.agents)\n",
        "        self.action_spaces = {agent: spaces.Discrete(6) for agent in self.agents}  # Add one for \"communicate\" action\n",
        "        self.observation_spaces = {agent: spaces.Dict({\n",
        "            \"grid\": spaces.Box(low=0, high=2, shape=(self.grid_size, self.grid_size, 3), dtype=np.float32),\n",
        "            \"state\": spaces.Dict({\n",
        "                \"position\": spaces.MultiDiscrete([self.grid_size, self.grid_size]),\n",
        "                \"water_timer\": spaces.Discrete(3),  # Timesteps until death without water\n",
        "                \"food_timer\": spaces.Discrete(7),  # Timesteps until death without food\n",
        "                \"beliefs\": spaces.Dict({\n",
        "                    \"water_replenish_rate\": spaces.Discrete(10),  # Example max rate\n",
        "                    \"food_replenish_rate\": spaces.Discrete(10),\n",
        "                })\n",
        "            })\n",
        "        }) for agent in self.agents}\n",
        "\n",
        "\n",
        "        self.grid = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.float32)  # Third dimension for agent presence, water, food\n",
        "        self.resource_counters = {\"water\": [np.inf, 0], \"food\": [np.inf, 0]}  # [acquisitions left, replenishment timer]\n",
        "        self.agent_states = {agent: {\"position\": None, \"water_timer\": 3, \"food_timer\": 7, \"beliefs\": {\"water_replenish_rate\": np.inf, \"food_replenish_rate\": np.inf}} for agent in self.agents}\n",
        "        self.current_agent = None\n",
        "        self.messages = {agent: \"\" for agent in self.agents}  # Initialize messages for each agent\n",
        "        # Initialize environment state variables\n",
        "\n",
        "    def reset(self):\n",
        "      self.agent_selector.reinit(self.agents)\n",
        "      self.current_agent = self.agent_selector.next()\n",
        "      self.grid *= 0  # Clear the grid\n",
        "\n",
        "      # Randomly place water and food, initialize resource counters\n",
        "      water_position = np.random.choice(self.grid_size**2)\n",
        "      food_position = np.random.choice(self.grid_size**2)\n",
        "      while food_position == water_position:\n",
        "          food_position = np.random.choice(self.grid_size**2)\n",
        "\n",
        "      self.grid[water_position // self.grid_size, water_position % self.grid_size, 1] = 1\n",
        "      self.grid[food_position // self.grid_size, food_position % self.grid_size, 2] = 1\n",
        "      self.resource_counters[\"water\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]  # Random example values\n",
        "      self.resource_counters[\"food\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]\n",
        "\n",
        "      # Set initial positions for agents and reset their states\n",
        "      for i, agent_id in enumerate(self.agents):\n",
        "          while True:\n",
        "              pos = np.random.choice(self.grid_size**2)\n",
        "              if self.grid[pos // self.grid_size, pos % self.grid_size].sum() == 0:  # Ensure the position is empty\n",
        "                  self.grid[pos // self.grid_size, pos % self.grid_size, 0] = i + 1  # Mark agent's presence\n",
        "                  agent = self.agents[i]  # Access agent by identifier\n",
        "                  self.agent_states[agent] = self.agent_states.get(agent, {})  # Ensure agent state exists\n",
        "                  self.agent_states[agent][\"position\"] = (pos // self.grid_size, pos % self.grid_size)\n",
        "                  self.agent_states[agent][\"water_timer\"] = 3\n",
        "                  self.agent_states[agent][\"food_timer\"] = 7\n",
        "                  break\n",
        "\n",
        "      # Return the initial observation for each agent using agent IDs as keys\n",
        "      return {agent_id: self.observe(agent_id) for agent_id in self.agents}\n",
        "\n",
        "    def step(self, actions):\n",
        "        rewards = {}\n",
        "        done = False\n",
        "        info = {}  # Additional info, if any\n",
        "\n",
        "        # Reset any necessary state (e.g., messages) at the start of a step\n",
        "        self.messages = {agent_id: \"\" for agent_id in self.agents}\n",
        "\n",
        "        for agent_id in self.agents:\n",
        "            # Ensure action is fetched using the agent_id\n",
        "            action_value = actions[agent_id]\n",
        "\n",
        "            reward = 0  # Initialize reward for this step\n",
        "\n",
        "            # Movement, consumption, or communication based on action_value\n",
        "            if action_value < 4:  # Movement actions\n",
        "                move_success = self.move_agent(agent_id, action_value)\n",
        "                reward -= 1 if move_success else 5  # Penalty for unsuccessful move\n",
        "            elif action_value == 4:  # Consumption action\n",
        "                reward += self.consume_resources(agent_id)\n",
        "            elif action_value == 5:  # Communication action\n",
        "                communicated_successfully = self.communicate(agent_id)\n",
        "                reward += 2 if communicated_successfully else -2  # Reward/penalty based on communication\n",
        "\n",
        "            # Update survival timers and check for termination conditions\n",
        "            self.agent_states[agent_id][\"water_timer\"] -= 1\n",
        "            self.agent_states[agent_id][\"food_timer\"] -= 1\n",
        "            if self.agent_states[agent_id][\"water_timer\"] <= 0 or self.agent_states[agent_id][\"food_timer\"] <= 0:\n",
        "                self.terminate_agent(agent_id)\n",
        "                reward -= 50  # Penalty for agent termination\n",
        "\n",
        "            # Survival reward\n",
        "            reward += 0.1\n",
        "\n",
        "            rewards[agent_id] = reward\n",
        "\n",
        "        done = len(self.agents) == 0  # Check if episode is done (all agents terminated)\n",
        "\n",
        "        # Construct return values using agent identifiers for keys\n",
        "        observations = {agent_id: self.observe(agent_id) for agent_id in self.agents}\n",
        "        return observations, rewards, done, info\n",
        "\n",
        "    def move_agent(self, agent, direction):\n",
        "        pos = self.agent_states[agent][\"position\"]\n",
        "        move_success = False  # Assume move is unsuccessful by default\n",
        "        if direction == 0:  # Up\n",
        "            new_pos = (max(pos[0] - 1, 0), pos[1])\n",
        "        elif direction == 1:  # Down\n",
        "            new_pos = (min(pos[0] + 1, self.grid_size - 1), pos[1])\n",
        "        elif direction == 2:  # Left\n",
        "            new_pos = (pos[0], max(pos[1] - 1, 0))\n",
        "        else:  # Right\n",
        "            new_pos = (pos[0], min(pos[1] + 1, self.grid_size - 1))\n",
        "\n",
        "        # Update position if the new position is not occupied\n",
        "        if self.grid[new_pos[0], new_pos[1], 0] == 0:\n",
        "            self.grid[pos[0], pos[1], 0] = 0  # Remove agent from old position\n",
        "            self.grid[new_pos[0], new_pos[1], 0] = 1  # Add agent to new position\n",
        "            self.agent_states[agent][\"position\"] = new_pos\n",
        "            move_success = True  # The move was successful\n",
        "        return move_success\n",
        "\n",
        "    def consume_resources(self, agent):\n",
        "      pos = self.agent_states[agent][\"position\"]\n",
        "      reward = 0\n",
        "      # Check for water\n",
        "      if self.grid[pos[0], pos[1], 1] == 1:\n",
        "          # Consume water if available\n",
        "          if self.resource_counters[\"water\"][0] > 0:  # If water is available\n",
        "              self.resource_counters[\"water\"][0] -= 1  # Decrement water availability\n",
        "              self.agent_states[agent][\"water_timer\"] = 3  # Reset water timer\n",
        "              reward += 10  # Reward for consuming water\n",
        "              if self.resource_counters[\"water\"][0] <= 0:  # If water is now depleted\n",
        "                  self.grid[pos[0], pos[1], 1] = 0  # Remove water from the grid\n",
        "      # Check for food\n",
        "      elif self.grid[pos[0], pos[1], 2] == 1:\n",
        "          # Consume food if available\n",
        "          if self.resource_counters[\"food\"][0] > 0:  # If food is available\n",
        "              self.resource_counters[\"food\"][0] -= 1  # Decrement food availability\n",
        "              self.agent_states[agent][\"food_timer\"] = 7  # Reset food timer\n",
        "              reward += 10  # Reward for consuming food\n",
        "              if self.resource_counters[\"food\"][0] <= 0:  # If food is now depleted\n",
        "                  self.grid[pos[0], pos[1], 2] = 0  # Remove food from the grid\n",
        "      return reward\n",
        "\n",
        "    def communicate(self, sender):\n",
        "      pos = self.agent_states[sender][\"position\"]\n",
        "      water_timer = self.agent_states[sender][\"water_timer\"]\n",
        "      food_timer = self.agent_states[sender][\"food_timer\"]\n",
        "      water_belief = self.agent_states[sender][\"beliefs\"][\"water_replenish_rate\"]\n",
        "      food_belief = self.agent_states[sender][\"beliefs\"][\"food_replenish_rate\"]\n",
        "\n",
        "      # Determine the urgency level for water and food based on remaining timers\n",
        "      water_urgency = \"high\" if water_timer <= 2 else \"low\"\n",
        "      food_urgency = \"high\" if food_timer <= 3 else \"low\"\n",
        "\n",
        "      message = {\n",
        "          \"location\": pos,\n",
        "          \"water_belief\": water_belief,\n",
        "          \"food_belief\": food_belief,\n",
        "          \"water_urgency\": water_urgency,\n",
        "          \"food_urgency\": food_urgency,\n",
        "          \"found\": None\n",
        "      }\n",
        "\n",
        "      communicated_successfully = False\n",
        "      # Check for the presence of water or food at the sender's location\n",
        "      if self.grid[pos[0], pos[1], 1] == 1:  # Water found\n",
        "          message[\"found\"] = \"water\"\n",
        "          communicated_successfully = True\n",
        "      elif self.grid[pos[0], pos[1], 2] == 1:  # Food found\n",
        "          message[\"found\"] = \"food\"\n",
        "          communicated_successfully = True\n",
        "\n",
        "      # If something was found, broadcast the message\n",
        "      if communicated_successfully:\n",
        "          for agent in self.agents:\n",
        "              if agent != sender:\n",
        "                  self.messages[agent] = message\n",
        "\n",
        "      return communicated_successfully\n",
        "\n",
        "    def terminate_agent(self, agent):\n",
        "        # logic for handling agent termination\n",
        "        self.agents.remove(agent)  # Remove the agent from the active list\n",
        "        self.grid[self.agent_states[agent][\"position\"][0], self.agent_states[agent][\"position\"][1], 0] = 0  # Clear the agent from the grid\n",
        "        del self.agent_states[agent]  # Remove the agent's state\n",
        "\n",
        "\n",
        "    def observe(self, agent):\n",
        "      # Return agent-specific observations including both grid and their internal state\n",
        "      observation = self.grid.copy()\n",
        "      agent_state = self.agent_states[agent]\n",
        "      observed_message = self.messages[agent]\n",
        "\n",
        "      # If there's a message, update beliefs based on the message\n",
        "      if observed_message:\n",
        "          if observed_message[\"found\"] == \"water\":\n",
        "              agent_state[\"beliefs\"][\"water_replenish_rate\"] = observed_message[\"water_belief\"]\n",
        "          elif observed_message[\"found\"] == \"food\":\n",
        "              agent_state[\"beliefs\"][\"food_replenish_rate\"] = observed_message[\"food_belief\"]\n",
        "\n",
        "      return {\"grid\": observation, \"state\": agent_state, \"message\": observed_message}\n",
        "\n",
        "    def update_resources(self):\n",
        "      # Iterate through each resource to update its status\n",
        "      for resource, counter in self.resource_counters.items():\n",
        "          if counter[0] <= 0:  # If depleted\n",
        "              counter[1] -= 1  # Decrement replenishment timer\n",
        "              if counter[1] <= 0:  # If it's time to replenish\n",
        "                  self.replenish_resource(resource)\n",
        "\n",
        "    def replenish_resource(self, resource):\n",
        "      # Randomly choose a new position for the resource\n",
        "      position = np.random.choice(self.grid_size**2)\n",
        "      if resource == \"water\":\n",
        "          self.grid[position // self.grid_size, position % self.grid_size, 1] = 1  # Place water\n",
        "          self.resource_counters[\"water\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]  # Reset counters\n",
        "      elif resource == \"food\":\n",
        "          self.grid[position // self.grid_size, position % self.grid_size, 2] = 1  # Place food\n",
        "          self.resource_counters[\"food\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]  # Reset counters\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        # Visualize the current state of the environment, including agent positions, resources, and timers\n",
        "        if mode == \"human\":\n",
        "          for r in range(self.grid_size):\n",
        "              print(\"+---\" * self.grid_size + \"+\")\n",
        "              for c in range(self.grid_size):\n",
        "                  cell = \" \"\n",
        "                  if self.grid[r, c, 0] > 0:  # Agent presence\n",
        "                      cell = \"A\"\n",
        "                  elif self.grid[r, c, 1] == 1:  # Water\n",
        "                      cell = \"W\"\n",
        "                  elif self.grid[r, c, 2] == 1:  # Food\n",
        "                      cell = \"F\"\n",
        "                  print(f\"| {cell} \", end=\"\")\n",
        "              print(\"|\")\n",
        "          print(\"+---\" * self.grid_size + \"+\")\n",
        "        elif mode == \"rgb_array\":\n",
        "          self.ax.clear()\n",
        "          self.ax.axis('off')\n",
        "\n",
        "          # Draw the grid\n",
        "          for r in range(self.grid_size):\n",
        "              for c in range(self.grid_size):\n",
        "                  if self.grid[r, c, 0] > 0:  # Agent presence\n",
        "                      self.ax.text(c, r, 'A', fontsize=12, ha='center', va='center', color='blue')\n",
        "                  elif self.grid[r, c, 1] == 1:  # Water\n",
        "                      self.ax.text(c, r, 'W', fontsize=12, ha='center', va='center', color='cyan')\n",
        "                  elif self.grid[r, c, 2] == 1:  # Food\n",
        "                      self.ax.text(c, r, 'F', fontsize=12, ha='center', va='center', color='green')\n",
        "\n",
        "          # Draw grid lines\n",
        "          self.ax.set_xticks(np.arange(-0.5, self.grid_size, 1), minor=True)\n",
        "          self.ax.set_yticks(np.arange(-0.5, self.grid_size, 1), minor=True)\n",
        "          self.ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=2)\n",
        "          self.ax.tick_params(which=\"minor\", size=0)\n",
        "\n",
        "          # Update the display\n",
        "          self.fig.canvas.draw()\n",
        "          plt.pause(0.01)  # Small delay to allow for real-time updating\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example action selection process\n",
        "actions = {agent_id: env.action_spaces[agent_id].sample() for agent_id in env.agents}\n",
        "print(f\"Actions: {actions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6OHSDuSwaBaY",
        "outputId": "1235657f-6f1d-4f1d-c37e-5c6601c37f97"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actions: {<__main__.MADDPGAgent object at 0x7de92cf31420>: 4, <__main__.MADDPGAgent object at 0x7de9428dca30>: 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = ForagingEnv()\n",
        "state = env.reset()\n",
        "print(f\"Initial State: {state}\")\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    actions = {agent: env.action_spaces[agent].sample() for agent in env.agents}  # Random actions for demonstration\n",
        "    print(f\"Actions: {actions}\")\n",
        "    next_state, rewards, done, info = env.step(actions)\n",
        "    print(f\"Next State: {next_state}, Rewards: {rewards}, Done: {done}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "qtu3P44gOlc3",
        "outputId": "060b1e23-4973-4aac-91d9-349c6ac794d4"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial State: {0: {'grid': array([[[0., 1., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [2., 0., 0.]]], dtype=float32), 'state': {'position': (1, 2), 'water_timer': 3, 'food_timer': 7, 'beliefs': {'water_replenish_rate': inf, 'food_replenish_rate': inf}}, 'message': ''}, 1: {'grid': array([[[0., 1., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 0., 0.]],\n",
            "\n",
            "       [[0., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [2., 0., 0.]]], dtype=float32), 'state': {'position': (2, 2), 'water_timer': 3, 'food_timer': 7, 'beliefs': {'water_replenish_rate': inf, 'food_replenish_rate': inf}}, 'message': ''}}\n",
            "Actions: {<__main__.MADDPGAgent object at 0x7de92cf31420>: 5, <__main__.MADDPGAgent object at 0x7de9428dca30>: 5}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'<' not supported between instances of 'MADDPGAgent' and 'int'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-be08a66a29fb>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_spaces\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# Random actions for demonstration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Actions: {actions}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Next State: {next_state}, Rewards: {rewards}, Done: {done}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-152-e5db7ad5bcd0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m           \u001b[0;31m# Assume an initial penalty for moving to encourage efficient movement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0maction_value\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Movement actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m               \u001b[0mmove_success\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m               \u001b[0mreward\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmove_success\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;31m# Penalize more if the move isn't successful (e.g., walking into a wall)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'MADDPGAgent' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment\n",
        "env = ForagingEnv()\n",
        "\n",
        "# Initialize agents with their actor and critic networks\n",
        "agents = [MADDPGAgent(obs_size=env.grid_size * env.grid_size * 3, action_size=env.action_size, n_agents=env.n_agents) for _ in range(env.n_agents)]\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # Assume agents can select actions based on observations\n",
        "    actions = [agents[agent_id].act(obs[agent]) for agent_id, agent in enumerate(env.agents)]\n",
        "    next_obs, rewards, done, _ = env.step(actions)\n",
        "    # Store (obs, actions, rewards, next_obs) in replay buffer\n",
        "    # Sample a batch from the replay buffer and update both actor and critic models\n",
        "    obs = next_obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "O2XogAzGukCr",
        "outputId": "de5ee773-61fd-4905-d874-bf187d610e35"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "<__main__.MADDPGAgent object at 0x7de92d1f2590>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-fca3d0f3460d>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Assume agents can select actions based on observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Store (obs, actions, rewards, next_obs) in replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-fca3d0f3460d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Assume agents can select actions based on observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Store (obs, actions, rewards, next_obs) in replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: <__main__.MADDPGAgent object at 0x7de92d1f2590>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment\n",
        "env = ForagingEnv()\n",
        "\n",
        "# Initialize agents with their actor and critic networks\n",
        "agents = [MADDPGAgent(obs_size=env.observation_size, action_size=env.action_size, n_agents=env.n_agents) for _ in range(env.n_agents)]\n",
        "\n",
        "# Initialize replay buffer\n",
        "replay_buffer = ReplayBuffer(capacity=10000)\n",
        "\n",
        "num_episodes = 100  # Specify the number of episodes\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        actions = [agent.act(state[agent_id]) for agent_id, agent in enumerate(agents)]\n",
        "        next_state, rewards, done, _ = env.step(actions)\n",
        "\n",
        "        # Store experience in replay buffer\n",
        "        replay_buffer.add(state, actions, rewards, next_state, done)\n",
        "\n",
        "        # Sample a batch of experiences from the buffer\n",
        "        batch = replay_buffer.sample(batch_size=64)  # Specify the batch size\n",
        "\n",
        "        # Update each agent - this involves updating both the actor and critic networks\n",
        "        for agent_id, agent in enumerate(agents):\n",
        "            agent.update(batch, agent_id, agents)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break"
      ],
      "metadata": {
        "id": "CSOf_RmSuoYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7g1wX1XuL6qL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}