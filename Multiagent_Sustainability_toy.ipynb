{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPinzyfXHadF+c7feVsdrSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahault/Multi-agent-sustainability/blob/main/Multiagent_Sustainability_toy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Environment Setup\n",
        "\n",
        "\n",
        "*   Grid Setup: The environment consists of a 3x3 grid.\n",
        "*   Agent Dynamics: Two agents that can move, communicate, and consume resources.\n",
        "*   Resource Dynamics: Water and food are placed randomly and can deplete and replenish.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uWGa5fvQOXcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pettingzoo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqdCeImROn8Y",
        "outputId": "56c5e90e-99ed-4c9f-dced-83033bfbdccf"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pettingzoo in /usr/local/lib/python3.10/dist-packages (1.24.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (1.25.2)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo) (0.29.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, obs_size, action_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, action_size),\n",
        "            nn.Tanh()  # Assuming actions are scaled between -1 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_size, action_size):\n",
        "        super(Critic, self).__init__()\n",
        "        # Input is concatenation of obs and actions for all agents\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size + action_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, actions):\n",
        "        return self.net(torch.cat([x, actions], dim=1))\n"
      ],
      "metadata": {
        "id": "hKabS05hgnKu"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MADDPGAgent:\n",
        "    def __init__(self, obs_size, action_size, n_agents):\n",
        "        self.actor = Actor(obs_size, action_size).float()  # Ensure the model is in float\n",
        "        self.critic = Critic(obs_size * n_agents, action_size * n_agents).float()  # Adjust according to the total number of agents\n",
        "\n",
        "        self.position = (0, 0)  # Example initial position\n",
        "        self.water_timer = 3\n",
        "        self.food_timer = 7\n",
        "        # Initialize beliefs, etc.\n",
        "        # Assume beliefs and other state variables are initialized similarly\n",
        "\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.01)  # Example learning rate\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.01)  # Example learning rate\n",
        "\n",
        "        self.target_actor = Actor(obs_size, action_size).float()\n",
        "        self.target_critic = Critic(obs_size * n_agents, action_size * n_agents).float()\n",
        "\n",
        "        # Initialize target networks with the same weights as the original networks\n",
        "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
        "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        action = self.actor(state)\n",
        "        return action.detach().numpy().flatten()\n",
        "    def update(self, batch, agent_id, agents, gamma=0.99, tau=0.01):\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "\n",
        "        # Get the number of agents\n",
        "        n_agents = len(agents)\n",
        "\n",
        "        # Get the current agent's state and action\n",
        "        state = states[agent_id]\n",
        "        action = actions[agent_id]\n",
        "\n",
        "        # Get the next state for the current agent\n",
        "        next_state = next_states[agent_id]\n",
        "\n",
        "        # Get the target actions for the next states using the target actor networks\n",
        "        next_actions = []\n",
        "        for i in range(n_agents):\n",
        "            next_action = agents[i].target_actor(next_states[i])\n",
        "            next_actions.append(next_action)\n",
        "        next_actions = torch.cat(next_actions, dim=1)\n",
        "\n",
        "        # Compute the target Q-value using the target critic network\n",
        "        target_q = agents[agent_id].target_critic(next_state, next_actions)\n",
        "        target_q = rewards[agent_id] + (gamma * target_q * (1 - dones[agent_id]))\n",
        "\n",
        "        # Compute the current Q-value using the critic network\n",
        "        current_q = agents[agent_id].critic(state, actions)\n",
        "\n",
        "        # Compute the critic loss using the TD error (Q-targets - Q-values)\n",
        "        critic_loss = F.mse_loss(current_q, target_q)\n",
        "\n",
        "        # Update the critic network by minimizing the critic loss\n",
        "        agents[agent_id].critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        agents[agent_id].critic_optimizer.step()\n",
        "\n",
        "        # Compute the actor loss using the critic network and the current actions\n",
        "        actor_loss = -agents[agent_id].critic(state, actions).mean()\n",
        "\n",
        "        # Update the actor network by minimizing the actor loss\n",
        "        agents[agent_id].actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        agents[agent_id].actor_optimizer.step()\n",
        "\n",
        "        # Perform soft updates of the target networks for both actor and critic\n",
        "        for target_param, param in zip(agents[agent_id].target_actor.parameters(), agents[agent_id].actor.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "        for target_param, param in zip(agents[agent_id].target_critic.parameters(), agents[agent_id].critic.parameters()):\n",
        "            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)"
      ],
      "metadata": {
        "id": "QW0aY6p1u-0P"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4GegEAtwOQIS"
      },
      "outputs": [],
      "source": [
        "from pettingzoo import AECEnv\n",
        "from pettingzoo.utils import agent_selector\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class ForagingEnv(AECEnv):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.grid_size = 3\n",
        "        self.n_agents = 2  # Number of agents\n",
        "        self.observation_size = self.grid_size * self.grid_size * 3 + 2\n",
        "        self.action_size = 6\n",
        "        self.agents = [MADDPGAgent(self.observation_size, self.action_size, self.n_agents) for _ in range(self.n_agents)]\n",
        "        self.agent_selector = agent_selector(self.agents)\n",
        "        self.action_spaces = {agent: spaces.Discrete(6) for agent in self.agents}  # Add one for \"communicate\" action\n",
        "        self.observation_spaces = {agent: spaces.Dict({\n",
        "            \"grid\": spaces.Box(low=0, high=2, shape=(self.grid_size, self.grid_size, 3), dtype=np.float32),\n",
        "            \"state\": spaces.Dict({\n",
        "                \"position\": spaces.MultiDiscrete([self.grid_size, self.grid_size]),\n",
        "                \"water_timer\": spaces.Discrete(3),  # Timesteps until death without water\n",
        "                \"food_timer\": spaces.Discrete(7),  # Timesteps until death without food\n",
        "                \"beliefs\": spaces.Dict({\n",
        "                    \"water_replenish_rate\": spaces.Discrete(10),  # Example max rate\n",
        "                    \"food_replenish_rate\": spaces.Discrete(10),\n",
        "                })\n",
        "            })\n",
        "        }) for agent in self.agents}\n",
        "\n",
        "\n",
        "        self.grid = np.zeros((self.grid_size, self.grid_size, 3), dtype=np.float32)  # Third dimension for agent presence, water, food\n",
        "        self.resource_counters = {\"water\": [np.inf, 0], \"food\": [np.inf, 0]}  # [acquisitions left, replenishment timer]\n",
        "        self.agent_states = {agent: {\"position\": None, \"water_timer\": 3, \"food_timer\": 7, \"beliefs\": {\"water_replenish_rate\": np.inf, \"food_replenish_rate\": np.inf}} for agent in self.agents}\n",
        "        self.current_agent = None\n",
        "        self.messages = {agent: \"\" for agent in self.agents}  # Initialize messages for each agent\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_selector.reinit(self.agents)\n",
        "        self.current_agent = self.agent_selector.next()\n",
        "        self.grid *= 0  # Clear the grid\n",
        "\n",
        "        # Randomly place water and food, initialize resource counters\n",
        "        water_position = np.random.choice(self.grid_size**2)\n",
        "        food_position = np.random.choice(self.grid_size**2)\n",
        "        while food_position == water_position:\n",
        "            food_position = np.random.choice(self.grid_size**2)\n",
        "\n",
        "        self.grid[water_position // self.grid_size, water_position % self.grid_size, 1] = 1\n",
        "        self.grid[food_position // self.grid_size, food_position % self.grid_size, 2] = 1\n",
        "        self.resource_counters[\"water\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]  # Random example values\n",
        "        self.resource_counters[\"food\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]\n",
        "\n",
        "        # Set initial positions for agents and reset their states\n",
        "        for i, agent in enumerate(self.agents):\n",
        "            while True:\n",
        "                pos = np.random.choice(self.grid_size**2)\n",
        "                if self.grid[pos // self.grid_size, pos % self.grid_size].sum() == 0:  # Ensure the position is empty\n",
        "                    self.grid[pos // self.grid_size, pos % self.grid_size, 0] = i + 1  # Mark agent's presence\n",
        "                    self.agent_states[agent][\"position\"] = (pos // self.grid_size, pos % self.grid_size)\n",
        "                    self.agent_states[agent][\"water_timer\"] = 3\n",
        "                    self.agent_states[agent][\"food_timer\"] = 7\n",
        "                    break\n",
        "\n",
        "    def step(self, action):\n",
        "      self.messages = {agent: \"\" for agent in self.agents}  # Reset messages\n",
        "      agent = self.current_agent\n",
        "      reward = 0  # Initialize reward for the current step\n",
        "\n",
        "      # Assume an initial penalty for moving to encourage efficient movement\n",
        "      if action < 4:  # Movement actions\n",
        "          move_success = self.move_agent(agent, action)\n",
        "          reward -= 1 if move_success else 5  # Penalize more if the move isn't successful (e.g., walking into a wall)\n",
        "      elif action == 4:  # Consumption action\n",
        "          reward += self.consume_resources(agent)\n",
        "      elif action == 5:  # Communication action\n",
        "          communicated_successfully = self.communicate(agent)\n",
        "          reward += 2 if communicated_successfully else -2  # Reward or penalize based on communication success\n",
        "\n",
        "      # Update survival timers and check conditions\n",
        "      self.agent_states[agent][\"water_timer\"] -= 1\n",
        "      self.agent_states[agent][\"food_timer\"] -= 1\n",
        "      if self.agent_states[agent][\"water_timer\"] <= 0 or self.agent_states[agent][\"food_timer\"] <= 0:\n",
        "          self.terminate_agent(agent)\n",
        "          reward -= 50  # Significant penalty for dying\n",
        "\n",
        "      # small survival reward for each timestep the agent survives without taking any specific action\n",
        "      reward += 0.1\n",
        "\n",
        "      # Move to the next agent\n",
        "      self.current_agent = self.agent_selector.next()\n",
        "\n",
        "      return reward\n",
        "\n",
        "    def move_agent(self, agent, direction):\n",
        "        pos = self.agent_states[agent][\"position\"]\n",
        "        move_success = False  # Assume move is unsuccessful by default\n",
        "        if direction == 0:  # Up\n",
        "            new_pos = (max(pos[0] - 1, 0), pos[1])\n",
        "        elif direction == 1:  # Down\n",
        "            new_pos = (min(pos[0] + 1, self.grid_size - 1), pos[1])\n",
        "        elif direction == 2:  # Left\n",
        "            new_pos = (pos[0], max(pos[1] - 1, 0))\n",
        "        else:  # Right\n",
        "            new_pos = (pos[0], min(pos[1] + 1, self.grid_size - 1))\n",
        "\n",
        "        # Update position if the new position is not occupied\n",
        "        if self.grid[new_pos[0], new_pos[1], 0] == 0:\n",
        "            self.grid[pos[0], pos[1], 0] = 0  # Remove agent from old position\n",
        "            self.grid[new_pos[0], new_pos[1], 0] = 1  # Add agent to new position\n",
        "            self.agent_states[agent][\"position\"] = new_pos\n",
        "            move_success = True  # The move was successful\n",
        "        return move_success\n",
        "\n",
        "    def consume_resources(self, agent):\n",
        "      pos = self.agent_states[agent][\"position\"]\n",
        "      reward = 0\n",
        "      # Check for water\n",
        "      if self.grid[pos[0], pos[1], 1] == 1:\n",
        "          # Consume water if available\n",
        "          if self.resource_counters[\"water\"][0] > 0:  # If water is available\n",
        "              self.resource_counters[\"water\"][0] -= 1  # Decrement water availability\n",
        "              self.agent_states[agent][\"water_timer\"] = 3  # Reset water timer\n",
        "              reward += 10  # Reward for consuming water\n",
        "              if self.resource_counters[\"water\"][0] <= 0:  # If water is now depleted\n",
        "                  self.grid[pos[0], pos[1], 1] = 0  # Remove water from the grid\n",
        "      # Check for food\n",
        "      elif self.grid[pos[0], pos[1], 2] == 1:\n",
        "          # Consume food if available\n",
        "          if self.resource_counters[\"food\"][0] > 0:  # If food is available\n",
        "              self.resource_counters[\"food\"][0] -= 1  # Decrement food availability\n",
        "              self.agent_states[agent][\"food_timer\"] = 7  # Reset food timer\n",
        "              reward += 10  # Reward for consuming food\n",
        "              if self.resource_counters[\"food\"][0] <= 0:  # If food is now depleted\n",
        "                  self.grid[pos[0], pos[1], 2] = 0  # Remove food from the grid\n",
        "      return reward\n",
        "\n",
        "    def communicate(self, sender):\n",
        "      pos = self.agent_states[sender][\"position\"]\n",
        "      water_timer = self.agent_states[sender][\"water_timer\"]\n",
        "      food_timer = self.agent_states[sender][\"food_timer\"]\n",
        "      water_belief = self.agent_states[sender][\"beliefs\"][\"water_replenish_rate\"]\n",
        "      food_belief = self.agent_states[sender][\"beliefs\"][\"food_replenish_rate\"]\n",
        "\n",
        "      # Determine the urgency level for water and food based on remaining timers\n",
        "      water_urgency = \"high\" if water_timer <= 2 else \"low\"\n",
        "      food_urgency = \"high\" if food_timer <= 3 else \"low\"\n",
        "\n",
        "      message = {\n",
        "          \"location\": pos,\n",
        "          \"water_belief\": water_belief,\n",
        "          \"food_belief\": food_belief,\n",
        "          \"water_urgency\": water_urgency,\n",
        "          \"food_urgency\": food_urgency,\n",
        "          \"found\": None\n",
        "      }\n",
        "\n",
        "      communicated_successfully = False\n",
        "      # Check for the presence of water or food at the sender's location\n",
        "      if self.grid[pos[0], pos[1], 1] == 1:  # Water found\n",
        "          message[\"found\"] = \"water\"\n",
        "          communicated_successfully = True\n",
        "      elif self.grid[pos[0], pos[1], 2] == 1:  # Food found\n",
        "          message[\"found\"] = \"food\"\n",
        "          communicated_successfully = True\n",
        "\n",
        "      # If something was found, broadcast the message\n",
        "      if communicated_successfully:\n",
        "          for agent in self.agents:\n",
        "              if agent != sender:\n",
        "                  self.messages[agent] = message\n",
        "\n",
        "      return communicated_successfully\n",
        "\n",
        "    def terminate_agent(self, agent):\n",
        "        # logic for handling agent termination\n",
        "        self.agents.remove(agent)  # Remove the agent from the active list\n",
        "        self.grid[self.agent_states[agent][\"position\"][0], self.agent_states[agent][\"position\"][1], 0] = 0  # Clear the agent from the grid\n",
        "        del self.agent_states[agent]  # Remove the agent's state\n",
        "\n",
        "\n",
        "    def observe(self, agent):\n",
        "      # Return agent-specific observations including both grid and their internal state\n",
        "      observation = self.grid.copy()\n",
        "      agent_state = self.agent_states[agent]\n",
        "      observed_message = self.messages[agent]\n",
        "\n",
        "      # If there's a message, update beliefs based on the message\n",
        "      if observed_message:\n",
        "          if observed_message[\"found\"] == \"water\":\n",
        "              agent_state[\"beliefs\"][\"water_replenish_rate\"] = observed_message[\"water_belief\"]\n",
        "          elif observed_message[\"found\"] == \"food\":\n",
        "              agent_state[\"beliefs\"][\"food_replenish_rate\"] = observed_message[\"food_belief\"]\n",
        "\n",
        "      return {\"grid\": observation, \"state\": agent_state, \"message\": observed_message}\n",
        "\n",
        "    def update_resources(self):\n",
        "      # Iterate through each resource to update its status\n",
        "      for resource, counter in self.resource_counters.items():\n",
        "          if counter[0] <= 0:  # If depleted\n",
        "              counter[1] -= 1  # Decrement replenishment timer\n",
        "              if counter[1] <= 0:  # If it's time to replenish\n",
        "                  self.replenish_resource(resource)\n",
        "\n",
        "    def replenish_resource(self, resource):\n",
        "      # Randomly choose a new position for the resource\n",
        "      position = np.random.choice(self.grid_size**2)\n",
        "      if resource == \"water\":\n",
        "          self.grid[position // self.grid_size, position % self.grid_size, 1] = 1  # Place water\n",
        "          self.resource_counters[\"water\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]  # Reset counters\n",
        "      elif resource == \"food\":\n",
        "          self.grid[position // self.grid_size, position % self.grid_size, 2] = 1  # Place food\n",
        "          self.resource_counters[\"food\"] = [np.random.randint(1, 5), np.random.randint(5, 15)]  # Reset counters\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        # Visualize the current state of the environment, including agent positions, resources, and timers\n",
        "        if mode == \"human\":\n",
        "          for r in range(self.grid_size):\n",
        "              print(\"+---\" * self.grid_size + \"+\")\n",
        "              for c in range(self.grid_size):\n",
        "                  cell = \" \"\n",
        "                  if self.grid[r, c, 0] > 0:  # Agent presence\n",
        "                      cell = \"A\"\n",
        "                  elif self.grid[r, c, 1] == 1:  # Water\n",
        "                      cell = \"W\"\n",
        "                  elif self.grid[r, c, 2] == 1:  # Food\n",
        "                      cell = \"F\"\n",
        "                  print(f\"| {cell} \", end=\"\")\n",
        "              print(\"|\")\n",
        "          print(\"+---\" * self.grid_size + \"+\")\n",
        "        elif mode == \"rgb_array\":\n",
        "          self.ax.clear()\n",
        "          self.ax.axis('off')\n",
        "\n",
        "          # Draw the grid\n",
        "          for r in range(self.grid_size):\n",
        "              for c in range(self.grid_size):\n",
        "                  if self.grid[r, c, 0] > 0:  # Agent presence\n",
        "                      self.ax.text(c, r, 'A', fontsize=12, ha='center', va='center', color='blue')\n",
        "                  elif self.grid[r, c, 1] == 1:  # Water\n",
        "                      self.ax.text(c, r, 'W', fontsize=12, ha='center', va='center', color='cyan')\n",
        "                  elif self.grid[r, c, 2] == 1:  # Food\n",
        "                      self.ax.text(c, r, 'F', fontsize=12, ha='center', va='center', color='green')\n",
        "\n",
        "          # Draw grid lines\n",
        "          self.ax.set_xticks(np.arange(-0.5, self.grid_size, 1), minor=True)\n",
        "          self.ax.set_yticks(np.arange(-0.5, self.grid_size, 1), minor=True)\n",
        "          self.ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=2)\n",
        "          self.ax.tick_params(which=\"minor\", size=0)\n",
        "\n",
        "          # Update the display\n",
        "          self.fig.canvas.draw()\n",
        "          plt.pause(0.01)  # Small delay to allow for real-time updating\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "env = ForagingEnv()\n",
        "env.reset()\n",
        "print(env.observe(env.agents[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtu3P44gOlc3",
        "outputId": "076ca75c-5a1a-49ac-eb74-0519b3502189"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'grid': array([[[2., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 1.]],\n",
            "\n",
            "       [[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 1., 0.]],\n",
            "\n",
            "       [[1., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]]], dtype=float32), 'state': {'position': (2, 0), 'water_timer': 3, 'food_timer': 7, 'beliefs': {'water_replenish_rate': inf, 'food_replenish_rate': inf}}, 'message': ''}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pseudocode for a single step in the environment\n",
        "obs = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # Assume agents can select actions based on observations\n",
        "    actions = [agent.act(obs[agent_id]) for agent_id, agent in enumerate(agents)]\n",
        "    next_obs, rewards, done, _ = env.step(actions)\n",
        "    # Store (obs, actions, rewards, next_obs) in replay buffer\n",
        "    # Sample a batch from the replay buffer and update both actor and critic models\n",
        "    obs = next_obs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "O2XogAzGukCr",
        "outputId": "9e3209b8-d14e-4d90-af01-040330dfd889"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'MADDPGAgent' object has no attribute 'act'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-9bd753a55f6e>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Assume agents can select actions based on observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Store (obs, actions, rewards, next_obs) in replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-59-9bd753a55f6e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Assume agents can select actions based on observations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnext_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Store (obs, actions, rewards, next_obs) in replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'MADDPGAgent' object has no attribute 'act'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize environment\n",
        "env = ForagingEnv()\n",
        "\n",
        "# Initialize agents with their actor and critic networks\n",
        "agents = [MADDPGAgent(obs_size=env.observation_size, action_size=env.action_size, n_agents=env.n_agents) for _ in range(env.n_agents)]\n",
        "\n",
        "# Initialize replay buffer\n",
        "replay_buffer = ReplayBuffer()\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        actions = [agent.act(state[agent_id]) for agent_id, agent in enumerate(agents)]\n",
        "        next_state, rewards, done, _ = env.step(actions)\n",
        "\n",
        "        # Store experience in replay buffer\n",
        "        replay_buffer.add(state, actions, rewards, next_state, done)\n",
        "\n",
        "        # Sample a batch of experiences from the buffer\n",
        "        batch = replay_buffer.sample()\n",
        "\n",
        "        # Update each agent - this involves updating both the actor and critic networks\n",
        "        for agent_id, agent in enumerate(agents):\n",
        "            agent.update(batch, agent_id, agents)\n",
        "\n",
        "        state = next_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "CSOf_RmSuoYN",
        "outputId": "3bb26755-1a38-4869-dbaa-59f2b4997a2b"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ReplayBuffer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-43991837f125>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Initialize replay buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mreplay_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReplayBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ReplayBuffer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7g1wX1XuL6qL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}